{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Techniques for Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[PyTorch Techniques for Model Optimization](#pytorch-techniques-for-model-optimization)\n",
    "\n",
    "- [Saving Progress w/ Model Checkpointing](##saving-progress-w-model-checkpointing)\n",
    "\n",
    "- [Model Training w/ Mini-Batches in PyTorch](##model-training-w-mini-batches-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Progress w/ Model Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Checkpointing Intro.** - Will now focus on model checkpointing using PyTorch. This is vital technique in machine learning that allows save state of model during training, ensuring best-performing models are preserved. Will come to understand how implement model checkpointing, allowing to save model whenever achieves best performance on a validation set.\n",
    "\n",
    "So model checkpointing involves saving state of a neural network model at various points during training process. Crucial for several reasons:\n",
    "- **Prevent Loss of Progress**: In case of unexpected interruptions (e.g., power failure, hardware consumption), checkpointing helps resuming training from last saved state.\n",
    "- **Save Best Performing Models**: By saving model whenever achieves a new best performance on validation set, ensure that retain best version of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up Environment** - Assume set up environment seen before and used below: import necessary libraries, do preprocessing of Wine dataset, define model, define loss and optimizer. Will for now omit training loop with eval, graphing of loss and finally saving model loading and confirming same val_loss, as this code will be modified with checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.utils as skUtils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "wine_set: skUtils.Bunch = load_wine()\n",
    "\n",
    "def load_preprocessed_data(wine: skUtils.Bunch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    X: np.ndarray = wine.data\n",
    "    y: np.ndarray = wine.target\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "    scaler: StandardScaler = StandardScaler().fit(Xtrain)\n",
    "    Xtrain_scaled: np.ndarray = scaler.transform(Xtrain)\n",
    "    Xtest_scaled: np.ndarray = scaler.transform(Xtest)\n",
    "\n",
    "    Xtrain_tensor: torch.Tensor = torch.tensor(Xtrain_scaled, dtype=torch.float32)\n",
    "    Xtest_tensor: torch.Tensor = torch.tensor(Xtest_scaled, dtype=torch.float32)\n",
    "    ytrain_tensor: torch.Tensor = torch.tensor(ytrain, dtype=torch.long)\n",
    "    ytest_tensor: torch.Tensor = torch.tensor(ytest, dtype=torch.long)\n",
    "\n",
    "    return Xtrain_tensor, Xtest_tensor, ytrain_tensor, ytest_tensor\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data(wine_set)\n",
    "\n",
    "model: nn.Sequential = nn.Sequential( # MAKE SURE YOU CAN CUSTOM DEFINE TOO!!\n",
    "    nn.Linear(in_features=13, out_features=10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "criterion: nn.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer: optim.Adam = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Checkpoint Parameters** - Before diving into training loops changes, first set up initial parameters for checkpointing. Will ensure can effectively track model's performance and save best version. Specifically need establish:\n",
    "- `best_loss` to keep track of best validation loss. Initialize `best_loss` to `float(inf)` to ensure first validation loss will trigger model save.\n",
    "- `checkpoint_path` where model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss: float = float('inf')\n",
    "checkpoint_path: str = \"best_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop with Checkpointing** - Now implement training loop portion with validation and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 0 with validation loss 1.1336\n",
      "At epoch 0/150, Loss is: 1.1362 and Val. Loss is: 1.1336\n",
      "Model saved at epoch 1 with validation loss 1.1296\n",
      "Model saved at epoch 2 with validation loss 1.1257\n",
      "Model saved at epoch 3 with validation loss 1.1219\n",
      "Model saved at epoch 4 with validation loss 1.1181\n",
      "Model saved at epoch 5 with validation loss 1.1143\n",
      "Model saved at epoch 6 with validation loss 1.1107\n",
      "Model saved at epoch 7 with validation loss 1.1071\n",
      "Model saved at epoch 8 with validation loss 1.1035\n",
      "Model saved at epoch 9 with validation loss 1.1000\n",
      "Model saved at epoch 10 with validation loss 1.0965\n",
      "At epoch 10/150, Loss is: 1.0989 and Val. Loss is: 1.0965\n",
      "Model saved at epoch 11 with validation loss 1.0931\n",
      "Model saved at epoch 12 with validation loss 1.0897\n",
      "Model saved at epoch 13 with validation loss 1.0862\n",
      "Model saved at epoch 14 with validation loss 1.0829\n",
      "Model saved at epoch 15 with validation loss 1.0796\n",
      "Model saved at epoch 16 with validation loss 1.0763\n",
      "Model saved at epoch 17 with validation loss 1.0729\n",
      "Model saved at epoch 18 with validation loss 1.0696\n",
      "Model saved at epoch 19 with validation loss 1.0662\n",
      "Model saved at epoch 20 with validation loss 1.0629\n",
      "At epoch 20/150, Loss is: 1.0641 and Val. Loss is: 1.0629\n",
      "Model saved at epoch 21 with validation loss 1.0595\n",
      "Model saved at epoch 22 with validation loss 1.0561\n",
      "Model saved at epoch 23 with validation loss 1.0527\n",
      "Model saved at epoch 24 with validation loss 1.0493\n",
      "Model saved at epoch 25 with validation loss 1.0458\n",
      "Model saved at epoch 26 with validation loss 1.0424\n",
      "Model saved at epoch 27 with validation loss 1.0389\n",
      "Model saved at epoch 28 with validation loss 1.0354\n",
      "Model saved at epoch 29 with validation loss 1.0318\n",
      "Model saved at epoch 30 with validation loss 1.0283\n",
      "At epoch 30/150, Loss is: 1.0298 and Val. Loss is: 1.0283\n",
      "Model saved at epoch 31 with validation loss 1.0248\n",
      "Model saved at epoch 32 with validation loss 1.0212\n",
      "Model saved at epoch 33 with validation loss 1.0177\n",
      "Model saved at epoch 34 with validation loss 1.0142\n",
      "Model saved at epoch 35 with validation loss 1.0107\n",
      "Model saved at epoch 36 with validation loss 1.0072\n",
      "Model saved at epoch 37 with validation loss 1.0036\n",
      "Model saved at epoch 38 with validation loss 1.0000\n",
      "Model saved at epoch 39 with validation loss 0.9964\n",
      "Model saved at epoch 40 with validation loss 0.9927\n",
      "At epoch 40/150, Loss is: 0.9930 and Val. Loss is: 0.9927\n",
      "Model saved at epoch 41 with validation loss 0.9889\n",
      "Model saved at epoch 42 with validation loss 0.9852\n",
      "Model saved at epoch 43 with validation loss 0.9813\n",
      "Model saved at epoch 44 with validation loss 0.9775\n",
      "Model saved at epoch 45 with validation loss 0.9736\n",
      "Model saved at epoch 46 with validation loss 0.9696\n",
      "Model saved at epoch 47 with validation loss 0.9656\n",
      "Model saved at epoch 48 with validation loss 0.9615\n",
      "Model saved at epoch 49 with validation loss 0.9574\n",
      "Model saved at epoch 50 with validation loss 0.9533\n",
      "At epoch 50/150, Loss is: 0.9516 and Val. Loss is: 0.9533\n",
      "Model saved at epoch 51 with validation loss 0.9491\n",
      "Model saved at epoch 52 with validation loss 0.9448\n",
      "Model saved at epoch 53 with validation loss 0.9405\n",
      "Model saved at epoch 54 with validation loss 0.9362\n",
      "Model saved at epoch 55 with validation loss 0.9318\n",
      "Model saved at epoch 56 with validation loss 0.9274\n",
      "Model saved at epoch 57 with validation loss 0.9230\n",
      "Model saved at epoch 58 with validation loss 0.9184\n",
      "Model saved at epoch 59 with validation loss 0.9139\n",
      "Model saved at epoch 60 with validation loss 0.9093\n",
      "At epoch 60/150, Loss is: 0.9049 and Val. Loss is: 0.9093\n",
      "Model saved at epoch 61 with validation loss 0.9046\n",
      "Model saved at epoch 62 with validation loss 0.8999\n",
      "Model saved at epoch 63 with validation loss 0.8952\n",
      "Model saved at epoch 64 with validation loss 0.8904\n",
      "Model saved at epoch 65 with validation loss 0.8855\n",
      "Model saved at epoch 66 with validation loss 0.8806\n",
      "Model saved at epoch 67 with validation loss 0.8757\n",
      "Model saved at epoch 68 with validation loss 0.8707\n",
      "Model saved at epoch 69 with validation loss 0.8657\n",
      "Model saved at epoch 70 with validation loss 0.8607\n",
      "At epoch 70/150, Loss is: 0.8537 and Val. Loss is: 0.8607\n",
      "Model saved at epoch 71 with validation loss 0.8556\n",
      "Model saved at epoch 72 with validation loss 0.8505\n",
      "Model saved at epoch 73 with validation loss 0.8453\n",
      "Model saved at epoch 74 with validation loss 0.8402\n",
      "Model saved at epoch 75 with validation loss 0.8350\n",
      "Model saved at epoch 76 with validation loss 0.8299\n",
      "Model saved at epoch 77 with validation loss 0.8247\n",
      "Model saved at epoch 78 with validation loss 0.8196\n",
      "Model saved at epoch 79 with validation loss 0.8144\n",
      "Model saved at epoch 80 with validation loss 0.8092\n",
      "At epoch 80/150, Loss is: 0.7997 and Val. Loss is: 0.8092\n",
      "Model saved at epoch 81 with validation loss 0.8040\n",
      "Model saved at epoch 82 with validation loss 0.7988\n",
      "Model saved at epoch 83 with validation loss 0.7937\n",
      "Model saved at epoch 84 with validation loss 0.7885\n",
      "Model saved at epoch 85 with validation loss 0.7833\n",
      "Model saved at epoch 86 with validation loss 0.7781\n",
      "Model saved at epoch 87 with validation loss 0.7729\n",
      "Model saved at epoch 88 with validation loss 0.7677\n",
      "Model saved at epoch 89 with validation loss 0.7626\n",
      "Model saved at epoch 90 with validation loss 0.7574\n",
      "At epoch 90/150, Loss is: 0.7443 and Val. Loss is: 0.7574\n",
      "Model saved at epoch 91 with validation loss 0.7523\n",
      "Model saved at epoch 92 with validation loss 0.7471\n",
      "Model saved at epoch 93 with validation loss 0.7420\n",
      "Model saved at epoch 94 with validation loss 0.7370\n",
      "Model saved at epoch 95 with validation loss 0.7319\n",
      "Model saved at epoch 96 with validation loss 0.7268\n",
      "Model saved at epoch 97 with validation loss 0.7218\n",
      "Model saved at epoch 98 with validation loss 0.7168\n",
      "Model saved at epoch 99 with validation loss 0.7118\n",
      "Model saved at epoch 100 with validation loss 0.7069\n",
      "At epoch 100/150, Loss is: 0.6897 and Val. Loss is: 0.7069\n",
      "Model saved at epoch 101 with validation loss 0.7019\n",
      "Model saved at epoch 102 with validation loss 0.6970\n",
      "Model saved at epoch 103 with validation loss 0.6921\n",
      "Model saved at epoch 104 with validation loss 0.6872\n",
      "Model saved at epoch 105 with validation loss 0.6824\n",
      "Model saved at epoch 106 with validation loss 0.6775\n",
      "Model saved at epoch 107 with validation loss 0.6727\n",
      "Model saved at epoch 108 with validation loss 0.6679\n",
      "Model saved at epoch 109 with validation loss 0.6631\n",
      "Model saved at epoch 110 with validation loss 0.6584\n",
      "At epoch 110/150, Loss is: 0.6379 and Val. Loss is: 0.6584\n",
      "Model saved at epoch 111 with validation loss 0.6537\n",
      "Model saved at epoch 112 with validation loss 0.6491\n",
      "Model saved at epoch 113 with validation loss 0.6445\n",
      "Model saved at epoch 114 with validation loss 0.6399\n",
      "Model saved at epoch 115 with validation loss 0.6354\n",
      "Model saved at epoch 116 with validation loss 0.6308\n",
      "Model saved at epoch 117 with validation loss 0.6264\n",
      "Model saved at epoch 118 with validation loss 0.6219\n",
      "Model saved at epoch 119 with validation loss 0.6175\n",
      "Model saved at epoch 120 with validation loss 0.6132\n",
      "At epoch 120/150, Loss is: 0.5899 and Val. Loss is: 0.6132\n",
      "Model saved at epoch 121 with validation loss 0.6089\n",
      "Model saved at epoch 122 with validation loss 0.6046\n",
      "Model saved at epoch 123 with validation loss 0.6004\n",
      "Model saved at epoch 124 with validation loss 0.5962\n",
      "Model saved at epoch 125 with validation loss 0.5921\n",
      "Model saved at epoch 126 with validation loss 0.5880\n",
      "Model saved at epoch 127 with validation loss 0.5840\n",
      "Model saved at epoch 128 with validation loss 0.5801\n",
      "Model saved at epoch 129 with validation loss 0.5762\n",
      "Model saved at epoch 130 with validation loss 0.5723\n",
      "At epoch 130/150, Loss is: 0.5462 and Val. Loss is: 0.5723\n",
      "Model saved at epoch 131 with validation loss 0.5685\n",
      "Model saved at epoch 132 with validation loss 0.5648\n",
      "Model saved at epoch 133 with validation loss 0.5611\n",
      "Model saved at epoch 134 with validation loss 0.5574\n",
      "Model saved at epoch 135 with validation loss 0.5538\n",
      "Model saved at epoch 136 with validation loss 0.5502\n",
      "Model saved at epoch 137 with validation loss 0.5467\n",
      "Model saved at epoch 138 with validation loss 0.5433\n",
      "Model saved at epoch 139 with validation loss 0.5398\n",
      "Model saved at epoch 140 with validation loss 0.5364\n",
      "At epoch 140/150, Loss is: 0.5073 and Val. Loss is: 0.5364\n",
      "Model saved at epoch 141 with validation loss 0.5331\n",
      "Model saved at epoch 142 with validation loss 0.5298\n",
      "Model saved at epoch 143 with validation loss 0.5265\n",
      "Model saved at epoch 144 with validation loss 0.5233\n",
      "Model saved at epoch 145 with validation loss 0.5201\n",
      "Model saved at epoch 146 with validation loss 0.5169\n",
      "Model saved at epoch 147 with validation loss 0.5138\n",
      "Model saved at epoch 148 with validation loss 0.5108\n",
      "Model saved at epoch 149 with validation loss 0.5077\n"
     ]
    }
   ],
   "source": [
    "num_epochs: int = 150\n",
    "history: Dict[str, List[float]] = {\"loss\": [], \"val_loss\": []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history[\"loss\"].append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test)\n",
    "        val_loss: float = criterion(outputs_test, y_test).item()\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model, checkpoint_path)\n",
    "        print(f'Model saved at epoch {epoch} with validation loss {val_loss:.4f}')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'At epoch {epoch}/{num_epochs}, Loss is: {loss.item():.4f} and Val. Loss is: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop:\n",
    "- Model is trained on training set\n",
    "- Model's performance is validated on the validation set\n",
    "- If validation loss improves, the model is saved using `torch.save()` . This ensures that only best performing model is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned concept and importance of model checkpointing, as well as how implement checkpointing in a PyTorch model. Remember that implementing effective checkpointing can significantly boost productivity and model performance in real-world machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training w/ Mini-Batches in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here learn how to efficiently train neural network model using mini-batches in PyTorch. Focus will be on understanding concept of mini-batches, creating them using PyTorch's `DataLoader` and training model using these mini-batches. Will be equipped with knowledge to implement mini-batch gradient descent in machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro to Mini-Batch Training** - In machine learning there are three main methods for training models: stochastic gradient descent (SGD), full-batch gradient descent, and mini-batch gradient descent. Explained here using simple analogy.\n",
    "\n",
    "Imagine learning to shoot basketballs in hoop:\n",
    "1. **Stochastic Gradient Descent (SGD)**: This is like shooting one basketball, adjusting your aim after each shot. Get feedback quickly, but each shot influenced by random factors, making learning process noisy.\n",
    "2. **Full-Batch Gradient Descent**: This is like shooting all basketballs you have, then reviewing overall performance to adjust your aim. Gives clear picture but is slow and tiring because have to shoot all balls before making any adjustments. \n",
    "3. **Mini-Batch Gradient Descent**: This method is middle ground. Like shooting few basketballs (say 10) before adjusting your aim. Faster than shooting all balls at once and more stable than adjusting after every single shot, offering balanced approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Use Mini-Batch Training?**\n",
    "- 1. **Efficiency**: Processing smaller subsets of data significantly reduces memory usage and can take advatntaged of parllel processing hardware.\n",
    "- 2. **Convergence**: Provides balance between noisy updates (SGD) and slow updates (full-batch), which can stabilize convergence.\n",
    "- 3. **Regularization**: Each mini-batch introduces some noise into parameter updates, which can help overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Dataset** -\n",
    "After having loaded dataset preprocessed and returned as PyTorch tensors, can use `DataLoader` to divide dataset into mini-batches and iterate over them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size: int = 32\n",
    "dataset: TensorDataset = TensorDataset(X_train, y_train)\n",
    "dataloader: DataLoader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code above:\n",
    "- `TensorDataset`: Combines features `X` and targets `y` into single dataset.\n",
    "- `DataLoader`: Splits dataset into mini-batches of size specified by `batch_size` , making it easy to iterate over dataset in chunks during training.\n",
    "\n",
    "By setting `batch_size=32` each mini-batch will contain 32 samples. The `shuffle=True` parameter ensures data shuffled at each epoch, improving the generalization capabilities of model. The `DataLoader` simplifies the process of batching and shuffling, which essential for efficient mini-batch training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building and Compiling Model** - Before using dataset split into mini-batches, the standard process of the model definition, loss function and optimizer are needed. These are copied from prev. section and doing `del` before to ensure data is reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, criterion, optimizer\n",
    "\n",
    "model: nn.Sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=13, out_features=10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "criterion: nn.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer: optim.Adam = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Model with Mini-Batches** - Now dataset, model, loss function and optimizer are ready, can begin training model using mini-batches. Here is training implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss: 1.1119\n",
      "Batch Loss: 1.1269\n",
      "Batch Loss: 1.1133\n",
      "Batch Loss: 1.1033\n",
      "Epoch [1/10] Average Loss: 1.1142\n",
      "Batch Loss: 1.1070\n",
      "Batch Loss: 1.1220\n",
      "Batch Loss: 1.1083\n",
      "Batch Loss: 1.0652\n",
      "Epoch [2/10] Average Loss: 1.1018\n",
      "Batch Loss: 1.0913\n",
      "Batch Loss: 1.0950\n",
      "Batch Loss: 1.0861\n",
      "Batch Loss: 1.0860\n",
      "Epoch [3/10] Average Loss: 1.0897\n",
      "Batch Loss: 1.1016\n",
      "Batch Loss: 1.0786\n",
      "Batch Loss: 1.0617\n",
      "Batch Loss: 1.0681\n",
      "Epoch [4/10] Average Loss: 1.0778\n",
      "Batch Loss: 1.0724\n",
      "Batch Loss: 1.0769\n",
      "Batch Loss: 1.0593\n",
      "Batch Loss: 1.0546\n",
      "Epoch [5/10] Average Loss: 1.0662\n",
      "Batch Loss: 1.0365\n",
      "Batch Loss: 1.0523\n",
      "Batch Loss: 1.0734\n",
      "Batch Loss: 1.0571\n",
      "Epoch [6/10] Average Loss: 1.0548\n",
      "Batch Loss: 1.0643\n",
      "Batch Loss: 1.0191\n",
      "Batch Loss: 1.0368\n",
      "Batch Loss: 1.0560\n",
      "Epoch [7/10] Average Loss: 1.0437\n",
      "Batch Loss: 1.0290\n",
      "Batch Loss: 1.0444\n",
      "Batch Loss: 1.0182\n",
      "Batch Loss: 1.0387\n",
      "Epoch [8/10] Average Loss: 1.0324\n",
      "Batch Loss: 1.0151\n",
      "Batch Loss: 1.0109\n",
      "Batch Loss: 1.0174\n",
      "Batch Loss: 1.0429\n",
      "Epoch [9/10] Average Loss: 1.0209\n",
      "Batch Loss: 1.0199\n",
      "Batch Loss: 0.9921\n",
      "Batch Loss: 1.0117\n",
      "Batch Loss: 1.0138\n",
      "Epoch [10/10] Average Loss: 1.0092\n"
     ]
    }
   ],
   "source": [
    "num_epochs: int = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss: float = 0.0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_X: torch.Tensor = batch[0]\n",
    "        batch_y: torch.Tensor = batch[1]\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'Batch Loss: {loss.item():.4f}')\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] Average Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now break down how works with mini-batches:\n",
    "\n",
    "1. **Iterating through Batches**: The loop `for batch_X, batch_y in dataloader` lets model process dataset in smaller chunks called mini-batches. Then `batch_X` contains input features, and `batch_y` contains corresponding labels for each mini-batch.\n",
    "\n",
    "2. **Calculating Batch Loss**: For each mini-batch, model makes predictions (`outputs`), and loss(`loss`) is computed by comparing these predictions to actual labels. The loss for each mini-batch is printed using `loss.item()`, providing immediate feedback.\n",
    "\n",
    "3. **Accumulating Loss**: Variable `running_loss` keeps cumulative total of loss for entire epoch. For each mini-batch, you add product of batch loss (`loss.item()`) and number of samples in that batch (`batch_X.size(0)`) to `running_loss` . This scaling is necessary because mini-batches can have different sizes, especially last mini-batch which might be smaller. By multiplying loss by the batch size, ensure each sample in dataset contributes equally to total loss, making final epoch loss calculation accurate.\n",
    "\n",
    "4. **Computing Epoch Loss**: At end of each epoch, average loss (`epoch_loss`) calculated by dividing `running_loss` by total number of samples in dataset(`len(dataloader.dataset)`). This gives normalized measure of loss over entire dataset, offering clear indication of model's performance for each epoch.\n",
    "\n",
    "Working with mini-batches helps make training process more efficient and provides way to handle larger datasets that wouldn't fit into memory all at once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
