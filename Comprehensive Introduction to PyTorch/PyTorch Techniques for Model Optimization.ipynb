{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Techniques for Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[PyTorch Techniques for Model Optimization](#pytorch-techniques-for-model-optimization)\n",
    "\n",
    "- [Saving Progress w/ Model Checkpointing](##saving-progress-w-model-checkpointing)\n",
    "\n",
    "- [Model Training w/ Mini-Batches in PyTorch](##model-training-w-mini-batches-in-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Progress w/ Model Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Checkpointing Intro.** - Will now focus on model checkpointing using PyTorch. This is vital technique in machine learning that allows save state of model during training, ensuring best-performing models are preserved. Will come to understand how implement model checkpointing, allowing to save model whenever achieves best performance on a validation set.\n",
    "\n",
    "So model checkpointing involves saving state of a neural network model at various points during training process. Crucial for several reasons:\n",
    "- **Prevent Loss of Progress**: In case of unexpected interruptions (e.g., power failure, hardware consumption), checkpointing helps resuming training from last saved state.\n",
    "- **Save Best Performing Models**: By saving model whenever achieves a new best performance on validation set, ensure that retain best version of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up Environment** - Assume set up environment seen before and used below: import necessary libraries, do preprocessing of Wine dataset, define model, define loss and optimizer. Will for now omit training loop with eval, graphing of loss and finally saving model loading and confirming same val_loss, as this code will be modified with checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.utils as skUtils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "wine_set: skUtils.Bunch = load_wine()\n",
    "\n",
    "def load_preprocessed_data(wine: skUtils.Bunch) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    X: np.ndarray = wine.data\n",
    "    y: np.ndarray = wine.target\n",
    "\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "    scaler: StandardScaler = StandardScaler().fit(Xtrain)\n",
    "    Xtrain_scaled: np.ndarray = scaler.transform(Xtrain)\n",
    "    Xtest_scaled: np.ndarray = scaler.transform(Xtest)\n",
    "\n",
    "    Xtrain_tensor: torch.Tensor = torch.tensor(Xtrain_scaled, dtype=torch.float32)\n",
    "    Xtest_tensor: torch.Tensor = torch.tensor(Xtest_scaled, dtype=torch.float32)\n",
    "    ytrain_tensor: torch.Tensor = torch.tensor(ytrain, dtype=torch.long)\n",
    "    ytest_tensor: torch.Tensor = torch.tensor(ytest, dtype=torch.long)\n",
    "\n",
    "    return Xtrain_tensor, Xtest_tensor, ytrain_tensor, ytest_tensor\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_preprocessed_data(wine_set)\n",
    "\n",
    "model: nn.Sequential = nn.Sequential( # MAKE SURE YOU CAN CUSTOM DEFINE TOO!!\n",
    "    nn.Linear(in_features=13, out_features=10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "criterion: nn.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer: optim.Adam = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize Checkpoint Parameters** - Before diving into training loops changes, first set up initial parameters for checkpointing. Will ensure can effectively track model's performance and save best version. Specifically need establish:\n",
    "- `best_loss` to keep track of best validation loss. Initialize `best_loss` to `float(inf)` to ensure first validation loss will trigger model save.\n",
    "- `checkpoint_path` where model will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss: float = float('inf')\n",
    "checkpoint_path: str = \"best_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop with Checkpointing** - Now implement training loop portion with validation and model checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs: int = 150\n",
    "history: Dict[str, List[float]] = {\"loss\": [], \"val_loss\": []}\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    history[\"loss\"].append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model(X_test)\n",
    "        val_loss: float = criterion(outputs_test, y_test).item()\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model, checkpoint_path)\n",
    "        print(f'Model saved at epoch {epoch} with validation loss {val_loss:.4f}')\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'At epoch {epoch}/{num_epochs}, Loss is: {loss.item():.4f} and Val. Loss is: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop:\n",
    "- Model is trained on training set\n",
    "- Model's performance is validated on the validation set\n",
    "- If validation loss improves, the model is saved using `torch.save()` . This ensures that only best performing model is saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learned concept and importance of model checkpointing, as well as how implement checkpointing in a PyTorch model. Remember that implementing effective checkpointing can significantly boost productivity and model performance in real-world machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training w/ Mini-Batches in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here learn how to efficiently train neural network model using mini-batches in PyTorch. Focus will be on understanding concept of mini-batches, creating them using PyTorch's `DataLoader` and training model using these mini-batches. Will be equipped with knowledge to implement mini-batch gradient descent in machine learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro to Mini-Batch Training** - In machine learning there are three main methods for training models: stochastic gradient descent (SGD), full-batch gradient descent, and mini-batch gradient descent. Explained here using simple analogy.\n",
    "\n",
    "Imagine learning to shoot basketballs in hoop:\n",
    "1. **Stochastic Gradient Descent (SGD)**: This is like shooting one basketball, adjusting your aim after each shot. Get feedback quickly, but each shot influenced by random factors, making learning process noisy.\n",
    "2. **Full-Batch Gradient Descent**: This is like shooting all basketballs you have, then reviewing overall performance to adjust your aim. Gives clear picture but is slow and tiring because have to shoot all balls before making any adjustments. \n",
    "3. **Mini-Batch Gradient Descent**: This method is middle ground. Like shooting few basketballs (say 10) before adjusting your aim. Faster than shooting all balls at once and more stable than adjusting after every single shot, offering balanced approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Use Mini-Batch Training?**\n",
    "- 1. **Efficiency**: Processing smaller subsets of data significantly reduces memory usage and can take advatntaged of parllel processing hardware.\n",
    "- 2. **Convergence**: Provides balance between noisy updates (SGD) and slow updates (full-batch), which can stabilize convergence.\n",
    "- 3. **Regularization**: Each mini-batch introduces some noise into parameter updates, which can help overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Dataset** -\n",
    "After having loaded dataset preprocessed and returned as PyTorch tensors, can use `DataLoader` to divide dataset into mini-batches and iterate over them efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size: int = 32\n",
    "dataset: TensorDataset = TensorDataset(X_train, y_train)\n",
    "dataloader: DataLoader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code above:\n",
    "- `TensorDataset`: Combines features `X` and targets `y` into single dataset.\n",
    "- `DataLoader`: Splits dataset into mini-batches of size specified by `batch_size` , making it easy to iterate over dataset in chunks during training.\n",
    "\n",
    "By setting `batch_size=32` each mini-batch will contain 32 samples. The `shuffle=True` parameter ensures data shuffled at each epoch, improving the generalization capabilities of model. The `DataLoader` simplifies the process of batching and shuffling, which essential for efficient mini-batch training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building and Compiling Model** - Before using dataset split into mini-batches, the standard process of the model definition, loss function and optimizer are needed. These are copied from prev. section and doing `del` before to ensure data is reset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, criterion, optimizer\n",
    "\n",
    "model: nn.Sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=13, out_features=10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "criterion: nn.CrossEntropyLoss = nn.CrossEntropyLoss()\n",
    "optimizer: optim.Adam = optim.Adam(model.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
