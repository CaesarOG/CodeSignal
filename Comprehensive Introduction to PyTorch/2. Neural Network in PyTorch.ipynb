{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Neural Network in PyTorch](#neural-network-in-pytorch)\n",
    "\n",
    "- [Initializing NN in PyTorch](##initializing-nn-in-pytorch)\n",
    "\n",
    "- [Networks via Sequential Model](##networks-via-sequential-model)\n",
    "\n",
    "- [Training an NN Model](##training-an-nn-model)\n",
    "\n",
    "- [Making Predictions with Trained Model](##making-predictions-with-trained-model)\n",
    "\n",
    "- [Evaluating Model with PyTorch](##evaluating-model-with-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing NN in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, we are going to explore how to initialize a basic Neural Network in PyTorch.\n",
    "\n",
    "Utilize PyTorch modules\n",
    "\n",
    "Build a simple neural network\n",
    "\n",
    "Define forward pass\n",
    "\n",
    "Instantiate the neural network model\n",
    "\n",
    "Print the model's architecture\n",
    "\n",
    "Before building a neural network, let's understand what a PyTorch module is.\n",
    "\n",
    "PyTorch’s modules are encapsulated as Python classes and serve as building blocks for designing models. The base to these is the nn.Module class. Any model you create in PyTorch is a subclass of the nn.Module. Here import PyTorch's `nn` module, base class for all neural network modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.layer1: nn.Linear = nn.Linear(in_features=2, out_features=10)\n",
    "        self.relu: nn.ReLU = nn.ReLU()\n",
    "        self.layer2: nn.Linear = nn.Linear(in_features=10, out_features=1)\n",
    "        self.sigmoid: nn.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        o: torch.Tensor = self.layer1(x)\n",
    "        o = self.relu(o)\n",
    "        o = self.layer2(o)\n",
    "        o = self.sigmoid(o)\n",
    "        return o\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code lays out structure of a simple neural network. First start by defining a class named `SimpleNN` which inherits from PyTorch's `nn.Module`. This inheritance allows `SimpleNN` to build on the robust features and functionalities provided by PyTorch.\n",
    "\n",
    "In the `__init__` method, calling `super(SimpleNN, self).__init__` ensures that `nn.Module`'s base properties are correctly initialized. This is crucial for setting up our neural network layers and activation functions properly.\n",
    "\n",
    "Next define first fully connected layer. This layer transforms an input with 2 features into an output with 10 features.\n",
    "\n",
    "Next, we add the ReLU (Rectified Linear Unit) activation function, introducing non-linearity after the first layer. This helps the model capture complex patterns in the data.\n",
    "\n",
    "Then second fully connected layer, input with 10 features and stay with output of 10 features. It's important to note that the input size of each layer must always match the output size of the preceding layer for the model to function correctly.\n",
    "\n",
    "Lastly, we include the Sigmoid activation function, which is applied after the second layer. This function maps the output to a range between 0 and 1, making it perfect for binary classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the forward pass, the `forward()` method of our `SimpleNN` class, we will define how the input tensor $x$ moves through the various layers and activation functions. This method will orchestrate the flow of data, specifying the sequence in which the layers and activation functions are applied to the input.\n",
    "\n",
    "The input tensor $x$ is initially the data we feed into the model, which has a size of 2 features (corresponding to `in_features=2` in the first layer). Let’s see how $x$ travels through the network:\n",
    "\n",
    "Initially, $x$ contains the input data with 2 features. It is first transformed by `layer1`, which increases it to 10 features. Then, the ReLU activation function is applied to introduce non-linear behavior. Next, the data moves to `layer2`, where it is reduced from 10 features to 1 feature. Finally, the Sigmoid activation function squashes the output to be between 0 and 1, making it suitable for binary classification tasks. The transformed $x$ is now the output of the model and is returned as the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: SimpleNN = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here creating an instance of `SimpleNN` class to represent model, instantiating it will run the `__init__` method , initializing it, so preparing it to be trained. When we now pass input data into `model`, this calls `forward` fn defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layer1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output details the layers included in `SimpleNN` model. Each layer is specified with its type, input and output dims. For example, `(layer1): Linear(in_features=2, out_features=10, bias=True)` indicates a linear layer that takes an input with 2 features and outputs 10 features, and it includes a bias term.\n",
    "\n",
    "So deepened understanding of a PyTorch model and how to initialize a basic Neural Network. We learned about PyTorch modules, building a simple Neural Network, defining a forward pass, instantiating a model, and printing the model's architecture.\n",
    "\n",
    "Work on initialize your own Neural Network model. Practice essential to cement learning and give you experience managing and creating PyTorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks via Sequential Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously initialized a neural network in PyTorch from scratch, a method that gives more flexibility for complex models. Now focus on a more efficient way of building the model using PyTorch's `nn.Sequential` model representation. Here learn to construct and inspect a Sequential Model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we delve into code, it's essential to understand the theoretical aspect. A Sequential Model, as the name suggests, allows us to build a model as a plain stack of layers — with each layer having one input tensor and one output tensor. This method of building models is extremely organized and efficient.\n",
    "\n",
    "The Sequential model is a linear stack of layers you can easily create using the `nn.Sequential()` function. It makes the building of models more comfortable, and the created models are shorter and more readable.\n",
    "\n",
    "If you recall from our previous example, when we build traditional neural networks, we define a custom class, add layers inside the constructor, and implement the forward method to define the computation steps. While this gives a lot of flexibility, it can be overkill for simple models. Here is where Sequential Models are handy — creating compact and understandable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here unravel how Sequential models are built in PyTorch, first initialize model as `nn.Sequential` which will contain all layers. Then add two linear layers with two activation functions (ReLU and Sigmoid) sequentially.\n",
    "\n",
    "In the first layer, `nn.Linear(2, 10)`, we define a Linear module with 2 input features and 10 output features. The second operation `nn.ReLU()` introduces a non-linearity between the layers. Next, we add another Linear layer `nn.Linear(10, 1)` that takes as input the output of the previous layer (10 input features) and also has 1 output feature. The last operation, `nn.Sigmoid()`, represents the Sigmoid activation function. It squashes the output between the range 0 and 1, which is useful for output neurons in the case of binary classification, like the case here.\n",
    "\n",
    "The print shows the structure of the model, its output shows model arch, describing each layer sequentually as added. Provides easily understood representation of hte model's structure, highlighting the layers and their configurations such as input and model sizes. Each operation performed in Sequential model is labelled with number, starting at zero. Operations are executed in order during forward computation/pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dive into the process of training a neural network model in PyTorch. See how to import necessary modules, prepare data, define a loss function or criterion, and an optimizer, and set up and run a training loop. By the end of this lesson, you’ll be able to train a neural network model using PyTorch.\n",
    "\n",
    "Our example will be training a neural network to predict if a soccer team is likely to win based on average goals scored by the team and average goals conceded by the opponent.\n",
    "\n",
    "Brief refresh on training NN - training a model is a process of learning the weight parameters that minimize the error on the training data. The process involves passing data through the model (forward propagation), computing the `loss` (how far the model's `prediction` is from the actual `value`), and then adjusting the weights using this loss (Backward Propagation).\n",
    "\n",
    "To do this in PyTorch, we will need our training data, a defined model, a loss function, and an optimizer for adjusting the weights. Example now to demonstrate concept vividly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the core of supervised learning techniques, we need input data (features) and output data (target/labels). In our scenario, the input represents the average goals scored by a soccer team and the average goals conceded by their opponent during the season. The output is binary, indicating whether the team is likely to win a match against this specific opponent (`1`) or not (`0`).\n",
    "\n",
    "Let's create our input and output tensor data using the `torch.tensor()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input features [Average Goals Scored, Average Goals Conceded by Opponent]\n",
    "X: torch.Tensor = torch.tensor([\n",
    "    [3.0, 0.5], [1.0, 1.0], [0.5, 2.0], [2.0, 1.5],\n",
    "    [3.5, 3.0], [2.0, 2.5], [1.5, 1.0], [0.5, 0.5],\n",
    "    [2.5, 0.8], [2.1, 2.0], [1.2, 0.5], [0.7, 1.5]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Target outputs [1 if the team is likely to win, 0 otherwise]\n",
    "y: torch.Tensor = torch.tensor([[1], [0], [0], [1], [1], [0], [1], [0], [1], [0], [1], [0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we've used `dtype=torch.float32` for both `X` and `y` as our loss function (Binary Cross-Entropy) requires the target tensor `y` to be in floating-point format. Other loss functions may require different data types, so it's crucial to ensure compatibility.\n",
    "\n",
    "We need to define our neural network model. For this binary classification task, we'll use a simple feedforward neural network with an input layer, one hidden layer, and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: nn.Sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=10, out_features=1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network model, we need to define a `criterion` (also known as a loss function) and an `optimizer`.\n",
    "\n",
    "The criterion measures how far the model's predictions are from the actual output. PyTorch provides several loss function classes, and for our binary classification task, we'll use the Binary Cross-Entropy (BCE) Loss.\n",
    "\n",
    "The optimizer is used to update the model parameters (weights and biases) based on the derivatives computed during backpropagation. PyTorch offers several optimization algorithms under `torch.optim`. In this example, we'll use Adam with a learning rate of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion: nn.BCELoss = nn.BCELoss()\n",
    "optimizer: optim.Adam = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up training loop - a neural network typically involves iteratively passing data through our model, calculating the loss, and backpropagating the loss to update our model. This process is repeated for a certain number of epochs. An epoch is one complete pass through the entire training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.17764712870121002\n",
      "loss:  0.039277028292417526\n",
      "loss:  0.010639287531375885\n",
      "loss:  0.004467851482331753\n",
      "loss:  0.0026919450610876083\n"
     ]
    }
   ],
   "source": [
    "# Train model for 50 epochs\n",
    "for epoch in range(50):\n",
    "    model.train() # Set the model to training mode\n",
    "\n",
    "    optimizer.zero_grad() # Zero the gradients for iteration\n",
    "\n",
    "    outputs = model(X) # Forward pass: compute predictions\n",
    "\n",
    "    loss = criterion(outputs, y) # Compute the loss\n",
    "\n",
    "    loss.backward() # Backward pass: compute the gradient of the loss\n",
    "\n",
    "    optimizer.step() # Optimize the model parameters based on the gradients\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(\"loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the training loop, the code runs a sequence of operations for each epoch, where an epoch is a single pass through entire dataset. Breakdown of each step:\n",
    "\n",
    "1. **Model Training Mode**: `model.train()` places the model in training mode, enabling necessary features that should only be active during training. It's good practice to keep it inside the loop to ensure the model is consistently set to training mode at the start of each epoch, especially if you might switch to other modes for some reason during the training process.\n",
    "\n",
    "2. **Reset Gradients**: `optimizer.zero_grad()` is called to reset the gradients of the model parameters; this is crucial because gradients accumulate by default in PyTorch.\n",
    "\n",
    "3. **Forward Pass**: `outputs = model(X)` computes the model's predictions based on the current state of the model parameters.\n",
    "\n",
    "4. **Loss Calculation**: The loss is calculated by comparing the model's predictions (`outputs`) to the true labels (`y`) using the pre-defined loss function criterion, which in this case is `nn.BCELoss()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following loss calculation, the backward pass is initiated:\n",
    "\n",
    "\n",
    "1. **Backward Pass**: `loss.backward()` computes the gradients of the loss with respect to each parameter. \n",
    "\n",
    "2. Parameter Update: The optimizer uses these gradients in `optimizer.step()` to adjust the model's parameters, reducing the loss for the next iteration.\n",
    "\n",
    "This sequence is repeated for a predefined number of epochs (`50` in this case), allowing the model to iteratively learn and improve its performance on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking and monitoring the training process - While training, want to keep track of loss at each epoch or certain intervals. Allows to see how model is improving over time of if it's struggling. Here every 10 epochs. The displayed `loss` values at various epochs indicate how the model's performance is improving over time as it learns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walked through the process of training a neural network model in PyTorch.\n",
    "- Define a loss function and an optimizer\n",
    "\n",
    "- Set up and run a training loop\n",
    "\n",
    "- Monitor your model's progress during training\n",
    "\n",
    "Reinforce understanding and mastery of these concepts with practice to apply what learned to new scenarios and solidify PyTorch proficiency.\n",
    "\n",
    "Successful machine learning is a process of iteration and refinement. First models won't be perfect don't get discouraged. With practice and experience, models and skills will continue to improve, keep learning and experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use trained PyTorch model to make predictions. Seen how to define and train neural network in PyTorch. Now utilize trained model useful way producing predictions. Hands on, using trained model to predict if a team is likely to win based on average goals scored by the team and average goals conceded by the opponent. Recall from above how train binary classification model, commit code to memory using both/either custom class and Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch Model into Evaluation Mode - The first crucial step after trained model is put into eval mode using `model.eval()` . Need to do because models can behave differently during training and evaluation phases. Many components or layers of model may have certain behaviors that only need occur during training, like backprop adjusting internal parameters based on the provided data. Putting model in evaluation mode ensures these components function correctly for making predictions. So do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (3): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind once finish predicting, if want do further training, will need set model back to training mode using `model.train()` before starting training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Model for Predictions - When making predictions, only need feed input data in to model and get outputs. Don't need to compute gradients because not updating the model's weights. As good practive, we can disable gradient calculation to save memory and computation. If don't disable for predictions it's not ideal, as can lead to significant issues in larger projects. PyTorch allows do this by wrapping prediction code block in `with torch.no_grad():`\n",
    "\n",
    "Here's code to create input tensor, disable gradient calculation, and make prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input: torch.Tensor = torch.tensor([[4., 5.]], dtype=torch.float32)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(new_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With model set to evaluation mode and no gradients, can feed model some input data and ask to make forward predictions. Input data should be of same type and shape model was trained with. Here `prediction` is output from forward pass of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting Model Outputs - Following forward pass, model will output raw values. These values dependent on architecture and final activation function of model. In example, output value in range $(0, 1)$ , representing probability of class being 1 because this simplified model has single output node with Sigmoid activation function. Can convert this probability to binary class using threshold, which usually $0.5$ , which will classify outputs greater than $0.5$ as class $1$. But if were to change probability threshold to $0.3$ , lowering threshold is like deciding team doesn't need very high chance to predicted as likely to win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output: tensor([[6.3508e-14]])\n",
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw output:\", prediction)\n",
    "\n",
    "print(\"Prediction:\", (prediction > 0.5).int().item())\n",
    "# boolean mask of tensor then to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here demonstrates model predicted a probability lower than threshold, ultimately classifying the input as class `0`. In scenario, this means model determined team is not likely to win match based on average goals scored by team and average goals conceded by opponent, showcasing how output probability is mapped to a class label based on specified threshold.\n",
    "\n",
    "This is how make predictions using a trained PyTorch model. Saw steps of transitioning model to evaluation mode, disabling gradient computation while using model to make predictions, and interpreting output of the model.\n",
    "\n",
    "By making trained model provide predictions, see results of all the training phase. Practice code to reinforce understanding each line so commit to memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dive into evaluating models in PyTorch. Evaluating performance of model plays key role in process of building an effective machine learning model. Helps understand ability of the model to generalize on unseen data. Attain this using a test dataset, making predictions using trained model, and comparing predictions with actual truth values test dataset.\n",
    "\n",
    "First summary of training steps:\n",
    "\n",
    "- **Data Preparation**: Defined training features `X_train` and targets `y_train`.\n",
    "\n",
    "- **Model Architecture**: Created a network using nn.Sequential with one hidden layer (ReLU) and an output layer (Sigmoid).\n",
    "\n",
    "- **Loss and Optimizer**: Used Binary Cross-Entropy Loss (BCELoss) and the Adam optimizer.\n",
    "\n",
    "- **Training Loop**: Trained for 50 epochs, performing forward pass, loss calculation, backpropagation, and parameter updates in each epoch.\n",
    "\n",
    "Now steps for evaluating model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Preparing Test Dataset - Before evaluating model, need to prepare test dataset. Test dataset consists of new data points model never seen before. Helps understand how well our model generalizes to unseen data.\n",
    "Now define our testing data with same format model was trained. With test data can move to evaluating model's performance with new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test: torch.Tensor = torch.tensor([[2.5, 1.0], [0.8, 0.8], [1.0, 2.0], [3.0, 2.5]], dtype=torch.float32)\n",
    "\n",
    "y_test: torch.Tensor = torch.tensor([[1], [0], [0], [1]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics - here see concept of evaluation metrics, which interpret performance of model. There are many metrics but first consider accuracy. Accuracy useful measure when target variable classes in data are nearly balanced. Defined as number of correct predictions made divided by total number predictions made. That is: $$\\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "Can easily apply this using Scikit-Learn. The `sklearn.metrics` module includes score functions, performance metrics, pairwise metrics and distance computations. Here import `accurary_score` from this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the accuracy_score functionality\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Model - Remember once model trained to switch with `model.eval()` . And then time to evaluate. Outline of steps:\n",
    "\n",
    "- Generate predictions from model\n",
    "\n",
    "- Convert predicted probabilities (0 to 1) to binary classes (0 and 1) using a threshold\n",
    "\n",
    "- Compute loss by comparing predictions with actual targets\n",
    "\n",
    "- Calculate accuracy of model on test data\n",
    "\n",
    "- Print test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 1.0, Test loss: 0.0013740158174186945\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    \n",
    "    predicted_classes: torch.Tensor = (outputs > 0.5).int()\n",
    "\n",
    "    test_loss = criterion(outputs, y_test).item()\n",
    "\n",
    "    test_accuracy = accuracy_score(y_test.numpy(), predicted_classes.numpy())\n",
    "\n",
    "print(f'\\nTest accuracy: {test_accuracy}, Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output implies model perfectly classified all test examples correctly with 100% accuracy. Very low test loss indicates model's predictions are very close to actual targets. However, important to note while perfect accuracy and low loss on test set desirable, don't always guarantee model will perform equally well on entirely new data. Overfitting can sometimes cause high performace on test set at expense of generalizability. Therefore essential to validate results with larger and more varied dataset in real-world applications.\n",
    "Now understand how evaluate model in PyTorch. Understanding model performance is core to building effective machine learning models, going forward, skills crucial for understanding and improving performance of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is another important metric, proportion of all positive classifications that actually positive: $$\\frac{TP}{TP + FP}$$\n",
    "It is also available in Scikit-Learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test precision: 1.0, Test loss: 0.0013740158174186945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_precision = precision_score(y_test.numpy(), predicted_classes.numpy())\n",
    "\n",
    "print(f'\\nTest precision: {test_precision}, Test loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
