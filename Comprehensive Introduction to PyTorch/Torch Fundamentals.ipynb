{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "[Torch Fundamentals](#torch-fundamentals)\n",
    "\n",
    "- [Torch Primitive Comps](##torch-primitive-comps)\n",
    "\n",
    "- [Tensor Reshaping](##tensor-reshaping)\n",
    "\n",
    "- [Torch Datasets](##torch-datasets)\n",
    "\n",
    "- [Torch Neural Layers](##torch-neural-layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Primitive Comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9, 11],\n",
       "        [13, 15]], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a: torch.Tensor = torch.tensor([[2, 3], [4, 5]], dtype=torch.int32)\n",
    "b: torch.Tensor = torch.tensor([[7, 8], [9, 10]], dtype=torch.int32)\n",
    "\n",
    "c: torch.Tensor = torch.tensor([[2], [3]], dtype=torch.int32)\n",
    "\n",
    "torch.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 24],\n",
       "        [36, 50]], dtype=torch.int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13],\n",
       "        [23]], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(a, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5],\n",
       "        [7, 8]], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  6],\n",
       "        [12, 15]], dtype=torch.int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "puli: torch.Tensor = torch.tensor([[2, 3], [4, 5]], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [4, 5]], dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig: torch.Tensor = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)\n",
    "orig # shape [2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped: torch.Tensor = orig.view(3, 2)\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened: torch.Tensor = orig.view(-1) # [6] is shape, 1-dim\n",
    "flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "orig.view(-1, 1) # or 6, 1 both give [6, 1] shape a col vec\n",
    "orig.view(1, 6) # gives [1, 6] a row vec\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat: torch.Tensor = orig.view(6, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now defining datasets using PyTorch Tensors. These datasets are called TensorDatasets and are a very vital feature of the PyTorch library. In this lesson, you will convert an array into a tensor, create a TensorDataset, use DataLoader for dividing the dataset into batches, and iterate through the batches. Let's dive right into it!\n",
    "What is a TensorDataset?\n",
    "\n",
    "As you might already know, PyTorch's primary unit of data storage is a tensor. But what if you have more than one tensor of data and you need to keep it collected? That's when TensorDataset comes into play.\n",
    "\n",
    "A TensorDataset is a dataset that wraps multiple tensors. Each sample is a tuple of tensors where each tensor in the tuple corresponds to a level of the dataset. In simpler terms, it is a way to keep your tensors of input and output data organized together. Using TensorDataset makes it very easy to provide and manage your tensors of varying data types.\n",
    "\n",
    "While itâ€™s not always necessary to use TensorDataset, it can be very convenient, especially if you want to use a DataLoader for batching and shuffling your data. The major advantage here is that using TensorDataset, PyTorch can efficiently store and access the data, which is crucial while working with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X: np.ndarray = np.array([[1., 2.], [2., 1.], [3., 4.], [4., 3.]])\n",
    "\n",
    "y: np.ndarray = np.array([0, 1, 0, 1])\n",
    "\n",
    "X_tensor: torch.Tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor: torch.Tensor = torch.tensor(y, dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just defined them as numpy arrays. We now have to convert them into PyTorch tensors.\n",
    "\n",
    "The conversion code is straightforward, the torch.tensor function helps us transform our numpy array into tensors, and, with the use of the dtype parameter, we can specify them as floating point and integer numbers.\n",
    "\n",
    "Now can build TensorDataset, the input to TensorDataset is the tensors we created above. TensorDataset will bundle or rather, wrap these tensors together into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]: tensor([1., 2.]), y[0]: 0\n",
      "X[1]: tensor([2., 1.]), y[1]: 1\n",
      "X[2]: tensor([3., 4.]), y[2]: 0\n",
      "X[3]: tensor([4., 3.]), y[3]: 1\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset: TensorDataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    X_sample, y_sample = dataset[i]\n",
    "    print(f\"X[{i}]: {X_sample}, y[{i}]: {y_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, helping in the effective management of large datasets and easier iterating over data batches, PyTorch provides a tool named DataLoader. It allows efficient access to data and can really speed up your model training process. Both TensorDataset and DataLoader if iterated both are a tuple of (data, target).\n",
    "\n",
    "DataLoader takes in a dataset and other parameters like batch_size, which defines the number of samples to work with per batch, and shuffle, which indicates to shuffle the data every epoch when set to True.\n",
    "\n",
    "Using a TensorDataset with DataLoader is highly convenient as it allows for seamless handling of inputs and targets together in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: tensor([[3., 4.],\n",
      "        [1., 2.]]), Batch y: tensor([0, 0], dtype=torch.int32)\n",
      "Batch X: tensor([[2., 1.],\n",
      "        [4., 3.]]), Batch y: tensor([1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataloader: DataLoader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for (batch_X, batch_y) in dataloader:\n",
    "    print(f\"Batch X: {batch_X}, Batch y: {batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So used the DataLoader and iterate through our dataset in batches. This process is fundamental in training Machine Learning models, as it allows the model to generalize better and also enables us to work with larger datasets by fitting only a batch of data in the memory at a time.\n",
    "\n",
    "This output illustrates how DataLoader allows us to shuffle and batch our data efficiently. Due to the shuffling, the presented batches and their order might vary each time the code is executed. This is beneficial for model generalization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now have a good understanding of defining PyTorch tensors and the convenience of using TensorDataset especially when paired with DataLoader. We also looked at iterating the DataLoader.\n",
    "Such situations with datasets commonly arise in a Machine Learning Engineer's daily work, hence proficiency in these skills is of utmost importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X: tensor([[1., 2.],\n",
      "        [3., 4.]]), Batch y: tensor([0, 0], dtype=torch.int32)\n",
      "Batch X: tensor([[4., 3.],\n",
      "        [2., 1.]]), Batch y: tensor([1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "X: np.ndarray = np.array([[1., 2.], [2., 1.], [3., 4.], [4., 3.]])\n",
    "y: np.ndarray = np.array([0, 1, 0, 1])\n",
    "\n",
    "X_tensor: torch.Tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor: torch.Tensor = torch.tensor(y, dtype=torch.int32)\n",
    "\n",
    "dataset: TensorDataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "dataloader: DataLoader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for (batch_X, batch_y) in dataloader:\n",
    "    print(f\"Batch X: {batch_X}, Batch y: {batch_y}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you make batch size 3 here, since there are only 4 examples, will get one batch with 3, other with only one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Neural Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue exploring tensor processing in PyTorch by discussing and implementing the crucial concepts of Linear Layers and Activation Functions.\n",
    "\n",
    "When working with tensors in neural networks, it is essential to understand that they are processed through various layers. A layer in a neural network refers to a collection of neurons (nodes) operating together at the same depth level within the network. PyTorch provides us with the `torch.nn` module, an easy and powerful tool for creating and organizing these layers.\n",
    "\n",
    "A vital part of most neural networks is the linear layer, which performs a linear transformation on its input data. A linear layer operates via the formula:\n",
    "\n",
    "$y = Wx + b$\n",
    "\n",
    "Where $y$ is the output, $W$ represents the weight matrix, $x$ is the input vector and $b$ is the bias vector. The weight matrix scales the input data, and the bias vector then shifts it, thereby producing the output. One of the powerful aspects of linear layers is their ability to transform the shape of the output as desired. By specifying the number of input and output features, you can control the dimensions of the tensor output from the layer. This flexibility allows the neural network to adapt to a variety of input shapes and deliver outputs that fit the requirements of the subsequent layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "inp: torch.Tensor = torch.tensor([[1., 2.]], dtype=torch.float32)\n",
    "layer: nn.Linear = nn.Linear(in_features=2, out_features=3)\n",
    "\n",
    "out: torch.Tensor = layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[1., 2.]])\n",
      "Layer: Linear(in_features=2, out_features=3, bias=True)\n",
      "Output: tensor([[ 1.4698,  0.2164, -1.3393]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input: {inp}\")\n",
    "print(f\"Layer: {layer}\")\n",
    "print(f\"Output: {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tensor displayed above results from passing the input tensor through the linear layer. This layer applies a weighted sum of the input tensor values and adds a bias term to each output. The weights and biases are initialized randomly, so the exact output can vary. The `grad_fn=<AddmmBackward0>` in the output means that PyTorch is keeping track of this operation, which will help compute the gradients automatically during model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now covering activation functions. Activation functions introduce non-linearity into the model, enabling it to handle more complex patterns in the data. Two commonly used activation functions are ReLU (Rectified Linear Unit) and Sigmoid.\n",
    "\n",
    "Mathematically, ReLU is represented as:\n",
    "\n",
    "$f(x) = max(0, x)$\n",
    "\n",
    "Where $x$ is the input to the function. The ReLU function ensures that positive input values remain unchanged, while negative ones are transformed to zero, creating a non-linear transformation.\n",
    "\n",
    "The Sigmoid function, on the other hand, is represented as:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "Where $x$ is the input. The Sigmoid function squashes the input value to lie between 0 and 1, which can be useful for binary classification tasks. However, in practice, ReLU is often preferred over Sigmoid for hidden layers due to its simplicity and performance benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.maximum(torch.zeros_like(x), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4698, 0.2164, 0.0000]], grad_fn=<MaximumBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReLU(layer(inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a ReLU activation function in PyTorch using the `nn.ReLU()` function from the `torch.nn module`. Then, it can be applied to the output tensor from our linear layer. The output tensor after activation will look just like the output of the raw implementation above.\n",
    "\n",
    "The output tensor after activation demonstrates the effect of the ReLU function. It zeroes out any negative values, converting them to zero, while keeping positive values unchanged. This introduces non-linearity into the model, which is crucial for handling more complex patterns in the data. The `grad_fn=<ReluBackward0>` shows that the ReLU operation is also being tracked for automatic differentiation during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output ReLU: tensor([[1.4698, 0.2164, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "output_relu: torch.Tensor = relu(out)\n",
    "print(f\"Output ReLU: {output_relu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can define and apply a Sigmoid activation function in PyTorch using the `nn.Sigmoid()` function from the `torch.nn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(x: torch.Tensor) -> torch.Tensor:\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8130, 0.5539, 0.2076]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigm(layer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Sigmoid: tensor([[0.8130, 0.5539, 0.2076]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "output_sigmoid: torch.Tensor = sigmoid(out)\n",
    "print(f\"Output Sigmoid: {output_sigmoid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "The output tensor after activation shows the effect of the Sigmoid function. It squashes the input values to lie between 0 and 1. This can be particularly useful in scenarios where you want to interpret the output as probabilities. The `grad_fn=<SigmoidBackward0>` indicates that the Sigmoid operation is tracked for automatic differentiation during training.\n",
    "\n",
    "Explored the concept of tensor processing through Linear Layers and Activation Functions in PyTorch. Also how to use these two functions in combination to transform and process an input tensor. solidifying your understanding of these concepts and your ability to process tensors effectively as you move forward in building more complex neural network architectures in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.]])\n",
      "tensor([[1.8252, 0.9872]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.8252, 0.9872]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.8612, 0.7285]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp: torch.Tensor = torch.tensor([[1., 2., 3.]], dtype=torch.float32) # example reducing din\n",
    "\n",
    "layer: nn.Linear = nn.Linear(in_features=3, out_features=2)\n",
    "\n",
    "relu: nn.ReLU = nn.ReLU()\n",
    "sigmoid: nn.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "print(inp)\n",
    "print(layer(inp))\n",
    "print(relu(layer(inp)))\n",
    "print(sigmoid(layer(inp)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
